{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5ff9da-77cb-434a-a3ac-9c1a5d2ff057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DUDAS\n",
    "\n",
    "# tenemos que usar convolusionales o LSTM? \n",
    "#que es el dense? \n",
    "# agregar features?\n",
    "# tenemos que normalizar?\n",
    "# nuestro input es el window size para el LSTM y la cantidad de caracteristicas? diferencia entre las convolusionales 2D?\n",
    "# si tenemos 2000 datos por feature, cuantas neuronas tenemos de input? 2000? nfeatures segun yo o mas bien, como relaciono el input shape con las neuronas?\n",
    "# segun yo cada feature representaba una neurona para la capa inicial, o es cada dato, o es independiente?\n",
    "# lo anterior como sería con convolusionales? los pixeles en convolusionales son features? explicar el por el video que vi\n",
    "# como sabemos que capa despues de la anterior, es decir LSTM y luego dropout o leaky relu o que onda\n",
    "# en que parte se cambia la funcion de costo? es decir cual es la diferencia entre como planteamos BinaryCrossentropy en GAN normal?\n",
    "# Para que sirve import tqdm?\n",
    "# Tarda mucho en correr?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85eb110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar en train y test el data set\n",
    "\n",
    "\n",
    "def download_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    data = data[['Adj Close']]  \n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    data = data.pct_change().dropna()\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    data_norm = (data - mean)/std \n",
    "    \n",
    "    return data, data_norm\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (50, 3)  # 50 pasos en el tiempo (ventana), 3 features\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "\n",
    "\n",
    "# Definir el generador\n",
    "def build_generator(latent_dim, seq_len, features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(seq_len, latent_dim)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(features))  # salida con el mismo número de características que la serie temporal\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_discriminator(seq_len, features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(seq_len, features)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # salida binaria para clasificar real o falso\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40336a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "ticker = \"MANU\"  \n",
    "start_date = '2014-10-29'\n",
    "end_date = '2024-10-30'\n",
    "data = download_data(ticker, start_date, end_date)\n",
    "data, data_norm = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5fd02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-30</th>\n",
       "      <td>-0.203636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>0.042260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-03</th>\n",
       "      <td>0.233881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-04</th>\n",
       "      <td>-0.720528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-05</th>\n",
       "      <td>-0.233994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close\n",
       "Date                 \n",
       "2014-10-30  -0.203636\n",
       "2014-10-31   0.042260\n",
       "2014-11-03   0.233881\n",
       "2014-11-04  -0.720528\n",
       "2014-11-05  -0.233994"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A como lo entiendo esto se mantendría igual lo que cambia del promedio de la resta es mas adelante\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37423cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(y_true, y_fake):\n",
    "    real_loss = loss_function(tf.ones_like(y_true), y_true)\n",
    "    fake_loss = loss_function(tf.zeros_like(y_fake), y_fake)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "def generator_loss(y_fake):\n",
    "    return loss_function(tf.ones_like(y_fake), y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa10dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, batch_size = 200):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = gen_model(noise, training = True)\n",
    "        \n",
    "        y_real = disc_model(images, training = True)\n",
    "        y_fake = disc_model(generated_images, training = True)\n",
    "        \n",
    "        gen_loss = generator_loss(-tf.math.reduce_mean(y_fake)) # o simplemente -tf.math.reduce_mean(y_fake) y sin las funciones de gen_loss y disc_loss\n",
    "        disc_loss = discriminator_loss(tf.reduce_mean(y_fake) - tf.reduce_mean(y_real)) #o simplemente tf.reduce_mean(y_fake) - tf.reduce_mean(y_real)\n",
    "        \n",
    "        \n",
    "    gradients_gen = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    gradients_disc = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_gen, gen_model.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_disc, disc_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    for i in tqdm.tqdm(range(num_batches)):\n",
    "        batch = x_train_norm[i*200:(1+i)*200]\n",
    "        train_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985403ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b86806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f90b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1 completado.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_sequences\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Generar 100 escenarios de prueba\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m scenarios \u001b[38;5;241m=\u001b[39m generate_scenarios(gen_model, num_scenarios\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Visualizar algunos de los escenarios generados\u001b[39;00m\n\u001b[0;32m     69\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[1;32mIn[12], line 62\u001b[0m, in \u001b[0;36mgenerate_scenarios\u001b[1;34m(gen_model, num_scenarios, sequence_length)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Desnormalizar los datos\u001b[39;00m\n\u001b[0;32m     61\u001b[0m generated_sequences \u001b[38;5;241m=\u001b[39m (generated_sequences \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Cambiar de rango [-1, 1] a [0, 1]\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m generated_sequences \u001b[38;5;241m=\u001b[39m generated_sequences \u001b[38;5;241m*\u001b[39m (data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_sequences\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Close'"
     ]
    }
   ],
   "source": [
    "# Instanciar el generador y el critic\n",
    "critic_model = critic()\n",
    "gen_model = generator()\n",
    "\n",
    "# Definir optimizadores\n",
    "generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "critic_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "# Clipping constante para los pesos del critic\n",
    "weight_clip = 0.01\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_data, batch_size=64):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "    # Entrenar el critic varias veces por cada paso del generador\n",
    "    for _ in range(5):\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            generated_sequences = gen_model(noise, training=True)\n",
    "            y_real = critic_model(real_data, training=True)\n",
    "            y_fake = critic_model(generated_sequences, training=True)\n",
    "            # Loss del critic basada en la distancia Wasserstein\n",
    "            disc_loss = tf.reduce_mean(y_fake) - tf.reduce_mean(y_real)\n",
    "        \n",
    "        gradients_disc = disc_tape.gradient(disc_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(gradients_disc, critic_model.trainable_variables))\n",
    "        \n",
    "        # Clipping de pesos para mantener la función 1-Lipschitz\n",
    "        for var in critic_model.trainable_variables:\n",
    "            var.assign(tf.clip_by_value(var, -weight_clip, weight_clip))\n",
    "\n",
    "    # Entrenar el generador\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_sequences = gen_model(noise, training=True)\n",
    "        y_fake = critic_model(generated_sequences, training=True)\n",
    "        # Loss del generador, que intenta maximizar la salida del critic\n",
    "        gen_loss = -tf.reduce_mean(y_fake)\n",
    "        \n",
    "    gradients_gen = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_gen, gen_model.trainable_variables))\n",
    "\n",
    "# Entrenar el modelo WGAN\n",
    "def train_wgan(train_data, epochs=1, batch_size=64):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(train_data) // batch_size):\n",
    "            batch = train_data[i*batch_size:(i+1)*batch_size]\n",
    "            batch = np.expand_dims(batch, -1)  # Añadir dimensión para compatibilidad con el modelo\n",
    "            train_step(batch)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} completado.\")\n",
    "\n",
    "# Entrenar el WGAN\n",
    "train_wgan(data_norm, epochs=1, batch_size=64)\n",
    "\n",
    "# Generar escenarios de prueba con el generador entrenado\n",
    "def generate_scenarios(gen_model, num_scenarios=100, sequence_length=50):\n",
    "    noise = tf.random.normal([num_scenarios, 100])\n",
    "    generated_sequences = gen_model(noise, training=False)\n",
    "    generated_sequences = generated_sequences.numpy()\n",
    "    # Desnormalizar los datos\n",
    "    generated_sequences = (generated_sequences + 1) / 2  # Cambiar de rango [-1, 1] a [0, 1]\n",
    "    generated_sequences = generated_sequences * (data['Close'].max() - data['Close'].min()) + data['Close'].min()\n",
    "    return generated_sequences\n",
    "\n",
    "# Generar 100 escenarios de prueba\n",
    "scenarios = generate_scenarios(gen_model, num_scenarios=100, sequence_length=50)\n",
    "\n",
    "# Visualizar algunos de los escenarios generados\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(5):  # Mostrar 5 ejemplos\n",
    "    plt.plot(scenarios[i], label=f'Scenario {i+1}')\n",
    "plt.title(\"Escenarios de Precios Generados por el WGAN para TSLA\")\n",
    "plt.xlabel(\"Días\")\n",
    "plt.ylabel(\"Precio\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f97cf9-36b9-4e23-96e7-9d0901c2fb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0c802-d9ae-44c1-bc09-f267560d8d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d792b948-c921-44b9-af5e-26b155f630f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scenarios' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Backtesting de la estrategia en los escenarios generados\u001b[39;00m\n\u001b[0;32m     23\u001b[0m all_portfolio_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scenario \u001b[38;5;129;01min\u001b[39;00m scenarios:\n\u001b[0;32m     25\u001b[0m     portfolio_values \u001b[38;5;241m=\u001b[39m simple_trading_strategy(scenario)\n\u001b[0;32m     26\u001b[0m     all_portfolio_values\u001b[38;5;241m.\u001b[39mappend(portfolio_values)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scenarios' is not defined"
     ]
    }
   ],
   "source": [
    "# Ejemplo de Estrategia Simple para Backtesting\n",
    "def simple_trading_strategy(prices, stop_loss=0.05, take_profit=0.1):\n",
    "    cash = 10000  # Capital inicial\n",
    "    position = 0  # No tenemos posición al inicio\n",
    "    portfolio_value = [cash]  # Valor del portafolio en el tiempo\n",
    "\n",
    "    for i in range(1, len(prices)):\n",
    "        if position == 0 and prices[i] > prices[i - 1]:  # Señal de compra\n",
    "            position = cash / prices[i]\n",
    "            cash = 0\n",
    "        elif position > 0:\n",
    "            # Calcular P&L en porcentaje\n",
    "            pnl = (prices[i] - prices[i - 1]) / prices[i - 1]\n",
    "            if pnl <= -stop_loss or pnl >= take_profit:  # Condiciones de venta\n",
    "                cash = position * prices[i]\n",
    "                position = 0\n",
    "        # Actualizar el valor del portafolio\n",
    "        portfolio_value.append(cash + position * prices[i])\n",
    "\n",
    "    return portfolio_value\n",
    "\n",
    "# Backtesting de la estrategia en los escenarios generados\n",
    "all_portfolio_values = []\n",
    "for scenario in scenarios:\n",
    "    portfolio_values = simple_trading_strategy(scenario)\n",
    "    all_portfolio_values.append(portfolio_values)\n",
    "\n",
    "# Visualizar el rendimiento de la estrategia en los escenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(5):  # Mostrar 5 ejemplos de rendimiento\n",
    "    plt.plot(all_portfolio_values[i], label=f'Scenario {i+1}')\n",
    "plt.title(\"Rendimiento de la Estrategia en Escenarios Generados para TSLA\")\n",
    "plt.xlabel(\"Días\")\n",
    "plt.ylabel(\"Valor del Portafolio\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e307d6-2bd9-4123-9133-59094eccdb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2648d-9ddd-4c6b-b60a-f207057e870e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27f9f8-f57c-4820-83a9-648223565546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e16581-4236-403c-b64d-e253049a4db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312e9d9-7756-4a3d-96a4-4926ddd49a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abf158-7f46-406d-9184-8e02c07a1de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
