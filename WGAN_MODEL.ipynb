{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5ff9da-77cb-434a-a3ac-9c1a5d2ff057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import ta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f6724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DUDAS\n",
    "\n",
    "\n",
    "# nuestro input es el window size para el LSTM y la cantidad de caracteristicas? diferencia entre las convolusionales 2D?\n",
    "# si tenemos 2000 datos por feature, cuantas neuronas tenemos de input? 2000? nfeatures segun yo o mas bien, como relaciono el input shape con las neuronas?\n",
    "# segun yo cada feature representaba una neurona para la capa inicial, o es cada dato, o es independiente?\n",
    "# lo anterior como sería con convolusionales? los pixeles en convolusionales son features? explicar el por el video que vi\n",
    "# Para que sirve import tqdm?\n",
    "# el mismo espacio de tiempo\n",
    "# mejorar la red neuronal. \n",
    "# Usar semillas para el random?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eb110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(ticker: str, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    data = data[['Adj Close']]  \n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    prices = data\n",
    "    r = (np.log(prices[['Adj Close']]/prices[['Adj Close']].shift(1))).dropna()\n",
    "    mean = r.mean()\n",
    "    std = r.std()\n",
    "    r_norm = (r - mean)/std \n",
    "    \n",
    "    return prices, r, r_norm, mean, std\n",
    "\n",
    "\n",
    "def generator(data: pd.DataFrame):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(data.shape[0], 1)))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(LeakyReLU(negative_slope=0.2) )\n",
    "    model.add(Dense(252))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(252, 1))) ## (252, 1)\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(data, batch_size = 100):\n",
    "    noise = tf.random.normal([batch_size, len(data), 1])\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = gen_model(noise, training = True)\n",
    "        \n",
    "        y_real = disc_model(data, training = True)\n",
    "        y_fake = disc_model(generated_data, training = True)\n",
    "        \n",
    "        gen_loss = -tf.math.reduce_mean(y_fake) # o simplemente -tf.math.reduce_mean(y_fake) y sin las funciones de gen_loss y disc_loss\n",
    "        disc_loss = tf.reduce_mean(y_fake) - tf.reduce_mean(y_real) #o simplemente tf.reduce_mean(y_fake) - tf.reduce_mean(y_real)\n",
    "        \n",
    "        \n",
    "    gradients_gen = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    gradients_disc = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_gen, gen_model.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_disc, disc_model.trainable_variables))\n",
    "    \n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, ticker: str, price: float, n_shares: int):\n",
    "        self.ticker = ticker\n",
    "        self.price = price\n",
    "        self.n_shares = n_shares\n",
    "        \n",
    "        \n",
    "def backtest(data: pd.DataFrame, sl: float, tp: float,\n",
    "             n_shares: int, rf = 0):\n",
    "    \n",
    "    \n",
    "    data = data.copy()\n",
    "\n",
    "    data.columns.values[0] = \"Close\"\n",
    "    \n",
    "    bollinger = ta.volatility.BollingerBands(data.Close, window=20)\n",
    "    data['BB_Buy'] = bollinger.bollinger_lband_indicator()\n",
    "\n",
    "    \n",
    "\n",
    "    capital = 1_000_000\n",
    "    COM = 0.125 / 100  # Commission percentage\n",
    "    active_long_positions = []\n",
    "    portfolio_value = [capital]\n",
    "     \n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "\n",
    "    # Iterar sobre los datos del mercado\n",
    "    for i, row in data.iterrows():\n",
    "        long_signal = row.BB_Buy  # Señal de compra\n",
    "\n",
    "        # Entrada de posición larga\n",
    "        if long_signal == True:\n",
    "            cost = row.Close * n_shares * (1 + COM)\n",
    "            if capital > cost and len(active_long_positions) < 100:\n",
    "                capital -= row.Close * n_shares * (1 + COM)\n",
    "                active_long_positions.append(\n",
    "                    Position(ticker=\"MANU\", price=row.Close, n_shares=n_shares))\n",
    "\n",
    "      \n",
    "\n",
    "        # Cierre de posiciones largas\n",
    "        for position in active_long_positions.copy():\n",
    "            if row.Close > position.price * (1 + tp):\n",
    "                capital += row.Close * position.n_shares * (1 - COM)\n",
    "                wins += 1  # Operación ganadora\n",
    "                active_long_positions.remove(position)\n",
    "            elif row.Close < position.price * (1 - sl):\n",
    "                capital += row.Close * position.n_shares * (1 - COM)\n",
    "                losses += 1  # Operación perdedora\n",
    "                active_long_positions.remove(position)\n",
    "        \n",
    "        value = capital + len(active_long_positions) * n_shares * row.Close       \n",
    "        portfolio_value.append(value)\n",
    "\n",
    "\n",
    "    # Convertir portfolio_value a una Serie de pandas\n",
    "    portfolio_series = pd.Series(portfolio_value)\n",
    "\n",
    "    # Calcular el rendimiento logarítmico\n",
    "    portafolio_value_rends = np.log(portfolio_series / portfolio_series.shift(1))\n",
    "    \n",
    "        # Calcular el Sharpe Ratio\n",
    "    mean_portfolio_return = portafolio_value_rends.mean()  # Rendimiento promedio del portafolio\n",
    "    portfolio_volatility = portafolio_value_rends.std()  # Volatilidad del portafolio\n",
    "    sharpe_ratio = (mean_portfolio_return - rf) / portfolio_volatility  # Sharpe Ratio\n",
    "\n",
    "    #print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "\n",
    "    # Calcular el valor máximo acumulado en cada momento\n",
    "    running_max = portfolio_series.cummax()\n",
    "\n",
    "    # Calcular el Drawdown\n",
    "    drawdown = (portfolio_series - running_max) / running_max\n",
    "\n",
    "    # Max Drawdown\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    #print(f\"Max Drawdown: {max_drawdown:.4f}\")\n",
    "\n",
    "    # Calcular el Win-Loss Ratio\n",
    "    if losses > 0:\n",
    "        win_loss_ratio = wins / losses\n",
    "    else:\n",
    "        win_loss_ratio = np.inf  # Si no hay pérdidas, el Win-Loss ratio es infinito\n",
    "\n",
    "    passive = list(data.Close)\n",
    "\n",
    "    #print(f\"Win-Loss Ratio: {win_loss_ratio:.2f}\")\n",
    "\n",
    "\n",
    "    calmar_value = calmar_ratio(portafolio_value_rends)\n",
    "    \n",
    "\n",
    "    return calmar_value, portfolio_series\n",
    "\n",
    "\n",
    "\n",
    "def calmar_ratio(returns):\n",
    "    # Calcula el retorno anualizado\n",
    "    annual_return = np.mean(returns) * 252  # 252 es el número promedio de días de mercado en un año\n",
    "    \n",
    "    # Calcula el drawdown máximo\n",
    "    cumulative_returns = (1 + returns).cumprod()  # Retorno acumulado\n",
    "    peak = cumulative_returns.cummax()  # Punto más alto\n",
    "    drawdown = (cumulative_returns - peak) / peak  # Pérdida desde el pico\n",
    "    max_drawdown = drawdown.min()  # Drawdown máximo\n",
    "    \n",
    "    # Calcula el Calmar Ratio\n",
    "    calmar_ratio = annual_return / abs(max_drawdown)\n",
    "    \n",
    "    return calmar_ratio\n",
    "\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40336a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "ticker = \"MANU\"  \n",
    "start_date = '2014-10-29'\n",
    "end_date = '2024-10-30'\n",
    "data = download_data(ticker, start_date, end_date)\n",
    "precios, data, data_norm, mean, std = preprocess_data(data)\n",
    "gen_model = generator(data_norm)\n",
    "disc_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b89e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:26<00:00,  2.22s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.56s/it]\n",
      "100%|██████████| 12/12 [00:19<00:00,  1.62s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.48s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.45s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.67s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.71s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.56s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.45s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.68s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.81s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.74s/it]\n",
      "100%|██████████| 12/12 [00:19<00:00,  1.62s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.46s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.68s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.58s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.47s/it]\n",
      "100%|██████████| 12/12 [00:19<00:00,  1.63s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.81s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.69s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.57s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.48s/it]\n",
      "100%|██████████| 12/12 [00:19<00:00,  1.61s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.88s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.85s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.76s/it]\n",
      "100%|██████████| 12/12 [00:19<00:00,  1.62s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.55s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.81s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.85s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.76s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.77s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.57s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.53s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.85s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.77s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.73s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.50s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.49s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.87s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.74s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.74s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.47s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.47s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.80s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.75s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.71s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.74s/it]\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.50s/it]\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.48s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.84s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.83s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:24<00:00,  2.02s/it]\n",
      "100%|██████████| 12/12 [00:25<00:00,  2.11s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.83s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.79s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.83s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.75s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.82s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.89s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.82s/it]\n",
      "100%|██████████| 12/12 [00:22<00:00,  1.85s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.72s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.70s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.74s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.80s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.72s/it]\n",
      "100%|██████████| 12/12 [00:21<00:00,  1.78s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.73s/it]\n",
      "100%|██████████| 12/12 [00:20<00:00,  1.69s/it]\n",
      "  8%|▊         | 1/12 [00:01<00:18,  1.70s/it]"
     ]
    }
   ],
   "source": [
    "gen_loss_history = []\n",
    "disc_loss_history = []\n",
    "\n",
    "num_batches = data_norm.shape[0] // 200\n",
    "for epoch in range(100):\n",
    "    for i in tqdm.tqdm(range(num_batches)):\n",
    "        batch = data_norm[i*200:(1+i)*200]\n",
    "        gen_loss, disc_loss = train_step(batch)\n",
    "\n",
    "        gen_loss_history.append(gen_loss.numpy())\n",
    "        disc_loss_history.append(disc_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_loss_history) # se tiene que acercar a cero. por que?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a777d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(disc_loss_history) # se tiene que alejar mas. por que?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_loss_history)\n",
    "plt.plot(disc_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49982260",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([100, 2000, 1])  \n",
    "\n",
    "generated_series = gen_model(noise, training=False)  \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for j in range(100):  \n",
    "    plt.plot(generated_series[j, :])\n",
    "\n",
    "plt.title(\"Rendientos generadas\")\n",
    "plt.xlabel(\"Tiempos\")\n",
    "plt.ylabel(\"Valores de rendimiento\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97843cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = generated_series.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b86806",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = precios['Adj Close'].sample(n=1).iloc[0]\n",
    "\n",
    "data_n = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    prices = [S0]\n",
    "    for log_return in scenario:\n",
    "        next_price = prices[-1] * np.exp(log_return)\n",
    "        prices.append(next_price)\n",
    "    data_n.append(prices)\n",
    "\n",
    "for prices in data_n:\n",
    "    plt.plot(prices, alpha=0.5, linewidth=0.75)\n",
    "\n",
    "plt.plot((precios[1760:]).values, label='Real Price', color='black', linewidth=1.5)\n",
    "plt.title('Simulated Prices vs Real Price')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_df = pd.DataFrame()\n",
    "for i in range(len(data_n)):\n",
    "    scenarios_df[f'Simulación {i+1}'] = data_n[i]\n",
    "    \n",
    "scenarios_df['Close'] = precios['Adj Close'].iloc[:253].values  # agregar precio original\n",
    "scenarios_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo 10 combinaciones de sl y tp\n",
    "combinations = [\n",
    "    (0.01, 0.02),\n",
    "    (0.01, 0.05),\n",
    "    (0.01, 0.08),\n",
    "    (0.02, 0.02),\n",
    "    (0.02, 0.05),\n",
    "    (0.02, 0.08),\n",
    "    (0.03, 0.02),\n",
    "    (0.03, 0.05),\n",
    "    (0.03, 0.08),\n",
    "    (0.025, 0.025)  # Agrega cualquier combinación adicional específica\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop para cada combinación en la lista predefinida\n",
    "for sl, tp in combinations:\n",
    "    calmar_ratios = []\n",
    "\n",
    "    # Ejecuta 10 simulaciones por combinación\n",
    "    num_simulations = len(scenarios)\n",
    "    for i in range(num_simulations):\n",
    "        # Ejecuta el backtest con la combinación actual\n",
    "        calmar, _ = backtest(scenarios_df.iloc[:, [i]], sl=sl, tp=tp, n_shares=20)\n",
    "        calmar_ratios.append(calmar)\n",
    "\n",
    "    # Calcula la media del Calmar Ratio para esta combinación\n",
    "    mean_calmar_ratio = np.mean(calmar_ratios)\n",
    "\n",
    "    # Guarda los resultados\n",
    "    results.append({\n",
    "        \"sl\": sl,\n",
    "        \"tp\": tp,\n",
    "        \"mean_calmar_ratio\": mean_calmar_ratio\n",
    "    })\n",
    "\n",
    "# Imprimir los resultados\n",
    "for result in results:\n",
    "    print(f\"SL: {result['sl']}, TP: {result['tp']}, Media del Calmar Ratio: {result['mean_calmar_ratio']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687696b",
   "metadata": {},
   "source": [
    "# el bollinger se aplica a cada serie o solo \n",
    "\n",
    "\n",
    "generated_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.save(\"./generador.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2 = tf.keras.models.load_model(\"./generador.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
